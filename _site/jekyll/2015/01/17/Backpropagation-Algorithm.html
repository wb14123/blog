<p>These days I start to learn neural networks again and write some Matlab codes from scratch. I try to understand everything I do while I write the code, so I derive the equations in the back propagation while try to keep it clear and easy to understand.</p>

<h2 id="neural-network-functions">Neural Network Functions</h2>

<p>A multi layer neural network could be defined as this:</p>

<div>
\begin{equation}
a_1(x, w, b) = x \\
\end{equation}

\begin{equation}
z_i(x, w, b) = a_{i-1}(x, w, b) \cdot w_{i-1} + b_{i-1}
\end{equation}

\begin{equation}
a_i(x, w, b) = \sigma(z_i(x, w, b))
\end{equation}

</div>

<p>Assume <span><script type="math/tex">layer_i</script></span> means the number of neural in layer i, then the variables in the equations could be explained as below:</p>

<ul>
  <li><code class="highlighter-rouge">x</code> is the input, which is a row vector, it has <span><script type="math/tex">layer_1</script></span> elements.</li>
  <li><span><script type="math/tex">w_i</script></span> meas the weights in layer <code class="highlighter-rouge">i</code>, which is a matrix of <span><script type="math/tex">layer_i</script></span> rows and <span><script type="math/tex">layer_{i+1}</script></span> columns.</li>
  <li><span><script type="math/tex">b_i</script></span> means biases in layer i, which is a row vector of <span><script type="math/tex">layer_{i+1}</script></span> elements.</li>
  <li><span><script type="math/tex">a_i</script></span> means activation function in the ith layer, which the output is a row vector, it has <span><script type="math/tex">layer_i</script></span> elements.</li>
  <li><code class="highlighter-rouge">l</code> means the last layer. The output of <span><script type="math/tex">a_l</script></span> is the output of the neural network.</li>
</ul>

<p>And <span><script type="math/tex">\sigma(z)</script></span> may be different in different use cases. This one is an example:</p>

<div>
\begin{equation}
\sigma(z) = {1 \over 1 + e^{-z}}
\end{equation}
</div>

<h2 id="gradient-descent-algorithm">Gradient Descent Algorithm</h2>

<p>We need a cost function to measure how well do we do for now. And the training of the network becomes a optimization problem. The method we use in the problem is gradient descent. Let me try to explain it.</p>

<p>Assume we have a cost function, and it is always non-negative. For example, this function is a good one:</p>

<div>
\begin{equation}
C(x, y, w, b) = {1 \over 2} (y - a_l(x, w, b)) ^ 2
\end{equation}
</div>

<p>Then the goal is try to make the output of the cost function smaller. Since <code class="highlighter-rouge">x</code> and <code class="highlighter-rouge">y</code> is fixed, the change of cost function while change <code class="highlighter-rouge">w</code> and <code class="highlighter-rouge">b</code> little could be shown as this:</p>

<div>
\begin{equation}
\Delta C = {\partial C \over \partial w} \Delta w + {\partial C \over \partial b} \Delta b 
\end{equation}
</div>

<p>In order to make the cost function smaller, we need to make <span><script type="math/tex">\Delta C</script></span> negative. We can make <span><script type="math/tex">\Delta w = - {\partial C \over \partial w}</script></span> and <span><script type="math/tex">\Delta b = - {\partial C \over \partial b}</script></span>. So that <span><script type="math/tex">\Delta C = - ({\partial C \over \partial w}) ^ 2 - ({\partial C \over \partial b}) ^ 2</script></span> which is always negative.</p>

<h2 id="compute-partial-derivative">Compute Partial Derivative</h2>

<p>So the goal is to compute the partial derivative <span><script type="math/tex">\partial C \over \partial w</script></span> and <span><script type="math/tex">\partial C \over \partial b</script></span>. Using equations (1), (2), (3), (5) and chain rule, we can get this:</p>

<div>

\begin{equation}
{\partial C \over \partial a_l} = a_l - y
\end{equation}

\begin{equation}
{\partial C \over \partial a_i} = {\partial C \over \partial a_{i+1}} {\partial a_{i+1} \over \partial \sigma} {\partial \sigma \over \partial z_{i+1}} {\partial z_{i+1} \over \partial a_i} = {\partial C \over \partial a_{i+1}} \odot {\sigma^{'}(z_{i+1})} w_i^{'}
\end{equation}

\begin{equation}
{\partial C \over \partial w_i} = {\partial C \over \partial a_{i+1}} {\partial a_{i+1} \over \partial \sigma} {\partial \sigma \over \partial z_{i+1}} {\partial z_{i+1} \over \partial w_i} = a_i^{'} {\partial C \over \partial a_{i+1}} \odot {\sigma^{'}(z_{i+1})}
\end{equation}

\begin{equation}
{\partial C \over \partial b_i} = {\partial C \over \partial a_{i+1}} {\partial a_{i+1} \over \partial \sigma} {\partial \sigma \over \partial z_{i+1}} {\partial z_{i+1} \over \partial b_i} = {\partial C \over \partial a_{i+1}} \odot {\sigma^{'}(z_{i+1})}
\end{equation}

</div>

<p>Note that we use <span><script type="math/tex">\odot</script></span> before <span><script type="math/tex">\sigma^{'}</script></span> is because function <span><script type="math/tex">\sigma(z)</script></span> is element wise.</p>

<p>We can find there are many same parts in these equations: <span><script type="math/tex">{\partial C \over \partial a_{i+1}} \odot {\sigma^{'}(z_{i+1})}</script></span>. So we can define <span><script type="math/tex">\delta_i = {\partial C \over \partial a_i} \odot {\sigma^{'}(z_i)}</script></span>, and rewrite these equations like this to avoid compute these parts many times:</p>

<div>
\begin{equation}
\delta_l = (a_l - y) \odot \sigma^{'}(z_l)
\end{equation}

\begin{equation}
\delta_i = \delta_{i+1} w_i^{'} \odot \sigma^{'}(z_i)
\end{equation}

\begin{equation}
{\partial C \over \partial w_i} = a_i^{'} \delta_{i+1}
\end{equation}

\begin{equation}
{\partial C \over \partial b_i} = \delta_{i+1}
\end{equation}
</div>

<p>With these equations, we can write the back propagation algorithm easily.</p>
